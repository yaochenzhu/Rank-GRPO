
# Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning

  

This repository contains the full training, alignment, and evaluation pipeline for **ConvRec-R1**, including both **Supervised Fine-Tuning (SFT)** and **Rank-GRPO** stages.  

## ⚙️ Environment Setup


```bash

# Create a dedicated conda environment
conda create -n rank-grpo python==3.10.0

# Activate the environment
conda  activate  rank-grpo

# Install core dependencies
uv pip install trl[vllm]==0.21.0
uv pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126
uv pip install vllm==0.10.0
uv pip install deepspeed bitsandbytes

# Install Rank-GRPO to trl
bash INSTALL.SH
```


## 🚀 Training Pipeline Overview

### **SFT Stage**

This stage fine-tunes a pretrained model (e.g., `Qwen2.5-0.5B-Instruct`) on conversational recommendation data.

Adjust the `configs/qwen25_0.5b_sft.yaml` file to ensure the **effective batch size** matches your GPU configuration.

#### Example Command

```bash

accelerate launch --config_file configs/qwen25_0.5b_sft.yaml  train_sft.py \
--dataset_path ./processed_datasets/sft_dataset  \
--model_name qwen/Qwen2.5-0.5B-Instruct \
--num_train_epochs 10  \
--learning_rate 5e-5 \
--warmup_ratio 0.05  \
--per_device_train_batch_size 12 \
--per_device_eval_batch_size 12  \
--gradient_accumulation_steps 8 \
--save_strategy  steps  \
--save_steps 100 \
--eval_strategy steps  \
--eval_steps 10 \
--logging_steps 10  \
2>&1 | tee logs/outputs_qwen25_0.5b_sft.txt
```

### **Rank-GRPO RL Alignment**

This stage aligns the SFT model using **offline RL with verifiable rewards** (Rank-GRPO). You can modify the config (`configs/sft_test_qwen25_0.5b.yaml`) to match your available hardware.

#### Example Command
```bash
accelerate launch --config_file  configs/sft_test_qwen25_0.5b.yaml  \
grpo_test_qwen25_0.5b_with_kl.py \
--train_path processed_datasets/grpo/grpo_dataset  \
--model_name qwen/Qwen2.5-0.5B-Instruct \
--sft_checkpoint 1500 \
--reward_func exp_inf \
--mu 1 \
--lr 1e-6 \
--kl_beta 1e-3 \
--adam_beta1 0.9 \
--adam_beta2 0.99 \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 16 \
--num_train_epochs 2 \
--gradient_accumulation_steps 6 \
--save_strategy steps \
--save_steps 200 \
--logging_steps 10 \
--use_vllm \
--vllm_mode colocate \
--vllm_gpu_memory_utilization 0.5 \
--vllm_tensor_parallel_size 4 \
--max_prompt_length 2048 \
--max_completion_length 1024 \
--num_generations 8 \
--seed 3407 \
--wandb_project rank_grpo \
--catalog_path gt_catalog.pkl \
2>&1 | tee logs/outputs_qwen25_0.5b_zero.txt
```
  

## 📊 Evaluation

Evaluation is divided into **validation** (training dynamics) and **test** (final model quality).

### **SFT Evaluation**


Plot training loss and validation metrics (NDCG / Recall).

```bash
python eval_sft.py \
--model_name Qwen/Qwen2.5-0.5B-Instruct \
--model_root ../results/Qwen/Qwen2.5-0.5B-Instruct  \
--dataset_path ../processed_datasets/sft_dataset \
--catalog_path ../processed_datasets/gt_catalog.pkl  \
--wandb_project sft_eval_val \
--upload_wandb
```
  
### **GRPO Validation Evaluation**

Track reward curves and validation metrics over training checkpoints.

```bash
python eval_grpo_val.py \
--model_name Qwen/Qwen2.5-0.5B-Instruct \
--model_root ../results/grpo/Qwen/Qwen2.5-0.5B-Instruct_lr1e-06_kl0.001 \
--dataset_path ../processed_datasets/sft_dataset \
--catalog_path ../processed_datasets/gt_catalog.pkl  \
--wandb_project grpo_eval_val \
--upload_wandb \
--step_interval 200
```

### **Final Evaluation on Test Set**

Evaluate the final checkpoint on the **held-out test set** using vLLM inference and compute top-K Recall / NDCG.

```bash
python grpo_test.py \
--model_name Qwen/Qwen2.5-0.5B-Instruct \
--model_root ../results/grpo/Qwen/Qwen2.5-0.5B-Instruct_lr1e-06_kl0.001 \
--dataset_path ../processed_datasets/sft_dataset \
--catalog_path ../processed_datasets/gt_catalog.pkl  \
--checkpoint 15800 \
2>&1 | tee logs/eval_grpo_test_qwen25_0.5b.txt
```
